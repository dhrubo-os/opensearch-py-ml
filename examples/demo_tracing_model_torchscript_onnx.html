

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Demo Notebook to trace Sentence Transformers model &mdash; Opensearch-py-ml 1.0.0b1 documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/plot_directive.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Demo Notebook for MLCommons Integration" href="demo_ml_commons_integration.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Opensearch-py-ml
          

          
          </a>

          
            
            
              <div class="version">
                1.0.0b1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../reference/index.html">API Reference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="demo_notebook.html">Demo Notebook for Dataframe</a></li>
<li class="toctree-l2"><a class="reference internal" href="online_retail_analysis.html">Online Retail analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="demo_transformer_model_train_save_upload_to_openSearch.html">Demo Notebook for Sentence Transformer Model Training, Saving and Uploading to OpenSearch</a></li>
<li class="toctree-l2"><a class="reference internal" href="demo_ml_commons_integration.html">Demo Notebook for MLCommons Integration</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Demo Notebook to trace Sentence Transformers model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Step-0:-Import-packages-and-set-up-client">Step 0: Import packages and set up client</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step-1:-Save-model-in-torchScript-format">Step 1: Save model in torchScript format</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step-2:-Upload-the-saved-torchScript-model-in-Opensearch">Step 2: Upload the saved torchScript model in Opensearch</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step-3:Save-model-in-Onnx-format">Step 3:Save model in Onnx format</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step-4:-Upload-the-saved-Onnx-model-in-Opensearch">Step 4: Upload the saved Onnx model in Opensearch</a></li>
<li class="toctree-l3"><a class="reference internal" href="#Step-5:-Generate-Sentence-Embedding">Step 5: Generate Sentence Embedding</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Opensearch-py-ml</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="index.html">Examples</a> &raquo;</li>
        
      <li>Demo Notebook to trace Sentence Transformers model</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/examples/demo_tracing_model_torchscript_onnx.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Demo-Notebook-to-trace-Sentence-Transformers-model">
<h1>Demo Notebook to trace Sentence Transformers model<a class="headerlink" href="#Demo-Notebook-to-trace-Sentence-Transformers-model" title="Permalink to this heading">¶</a></h1>
<p>This notebook provides a walkthrough guidance for users to trace models from Sentence Transformers in torchScript and onnx format. After tracing the model customer can upload the model to opensearch and generate embeddings.</p>
<p>Remember, tracing model in torchScript or Onnx format at just two different options. We don’t need to trace model in both ways. Here in our notebook we just want to show both ways.</p>
<p>Step 0: Import packages and set up client</p>
<p>Step 1: Save model in torchScript format</p>
<p>Step 2: Upload the saved torchScript model in Opensearch</p>
<p>[The following steps are optional, just showing uploading model in both ways and comparing the both embedding output]</p>
<p>Step 3: Save model in Onnx format</p>
<p>Step 4: Upload the saved Onnx model in Opensearch</p>
<p>Step 5: Generate Sentence Embedding with uploaded models</p>
<section id="Step-0:-Import-packages-and-set-up-client">
<h2>Step 0: Import packages and set up client<a class="headerlink" href="#Step-0:-Import-packages-and-set-up-client" title="Permalink to this heading">¶</a></h2>
<p>Install required packages for opensearch_py_ml.sentence_transformer_model Install <code class="docutils literal notranslate"><span class="pre">opensearchpy</span></code> and <code class="docutils literal notranslate"><span class="pre">opensearch-py-ml</span></code> through pypi</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="ch">#!pip install opensearch-py opensearch-py-ml</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">DeprecationWarning</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;Unverified HTTPS request&quot;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">opensearch_py_ml</span> <span class="k">as</span> <span class="nn">oml</span>
<span class="kn">from</span> <span class="nn">opensearchpy</span> <span class="kn">import</span> <span class="n">OpenSearch</span>
<span class="kn">from</span> <span class="nn">opensearch_py_ml.ml_models</span> <span class="kn">import</span> <span class="n">SentenceTransformerModel</span>
<span class="c1"># import mlcommon to later upload the model to OpenSearch Cluster</span>
<span class="kn">from</span> <span class="nn">opensearch_py_ml.ml_commons</span> <span class="kn">import</span> <span class="n">MLCommonClient</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">CLUSTER_URL</span> <span class="o">=</span> <span class="s1">&#39;https://localhost:9200&#39;</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_os_client</span><span class="p">(</span><span class="n">cluster_url</span> <span class="o">=</span> <span class="n">CLUSTER_URL</span><span class="p">,</span>
                  <span class="n">username</span><span class="o">=</span><span class="s1">&#39;admin&#39;</span><span class="p">,</span>
                  <span class="n">password</span><span class="o">=</span><span class="s1">&#39;admin&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Get OpenSearch client</span>
<span class="sd">    :param cluster_url: cluster URL like https://ml-te-netwo-1s12ba42br23v-ff1736fa7db98ff2.elb.us-west-2.amazonaws.com:443</span>
<span class="sd">    :return: OpenSearch client</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">OpenSearch</span><span class="p">(</span>
        <span class="n">hosts</span><span class="o">=</span><span class="p">[</span><span class="n">cluster_url</span><span class="p">],</span>
        <span class="n">http_auth</span><span class="o">=</span><span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">),</span>
        <span class="n">verify_certs</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">client</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">client</span> <span class="o">=</span> <span class="n">get_os_client</span><span class="p">()</span>

<span class="c1">#connect to ml_common client with OpenSearch client</span>
<span class="kn">import</span> <span class="nn">opensearch_py_ml</span> <span class="k">as</span> <span class="nn">oml</span>
<span class="kn">from</span> <span class="nn">opensearch_py_ml.ml_commons</span> <span class="kn">import</span> <span class="n">MLCommonClient</span>
<span class="n">ml_client</span> <span class="o">=</span> <span class="n">MLCommonClient</span><span class="p">(</span><span class="n">client</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Step-1:-Save-model-in-torchScript-format">
<h2>Step 1: Save model in torchScript format<a class="headerlink" href="#Step-1:-Save-model-in-torchScript-format" title="Permalink to this heading">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">Opensearch-py-ml</span></code> plugin provides method <code class="docutils literal notranslate"><span class="pre">save_as_pt</span></code> which will trace a model in torchScript format and save the model in a zip file in your filesystem.</p>
<p>Detailed documentation: <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.save_as_pt.html#opensearch_py_ml.ml_models.SentenceTransformerModel.save_as_pt">https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.save_as_pt.html#opensearch_py_ml.ml_models.SentenceTransformerModel.save_as_pt</a></p>
<p>Users need to provide a model id from sentence transformers (an example: <code class="docutils literal notranslate"><span class="pre">sentence-transformers/all-MiniLM-L6-v2</span></code>). This model id is a huggingface model id. Exaample: <a class="reference external" href="https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2">https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2</a></p>
<p><code class="docutils literal notranslate"><span class="pre">save_as_pt</span></code> will download the model in filesystem and then trace the model with the given input strings.</p>
<p>To get more direction about dummy input string please check this url: <a class="reference external" href="https://huggingface.co/docs/transformers/torchscript#dummy-inputs-and-standard-lengths">https://huggingface.co/docs/transformers/torchscript#dummy-inputs-and-standard-lengths</a></p>
<p>after tracing the model (a .pt file will be generated), <code class="docutils literal notranslate"><span class="pre">save_as_pt</span></code> method zips <code class="docutils literal notranslate"><span class="pre">tokenizers.json</span></code> and torchScript (<code class="docutils literal notranslate"><span class="pre">.pt</span></code>) file and saves in the file system.</p>
<p>User can upload that model to opensearch to generate embedding.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pre_trained_model</span> <span class="o">=</span> <span class="n">SentenceTransformerModel</span><span class="p">(</span><span class="n">folder_path</span> <span class="o">=</span> <span class="s1">&#39;/Volumes/workplace/upload_content/&#39;</span><span class="p">,</span> <span class="n">overwrite</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="n">pre_trained_model</span><span class="o">.</span><span class="n">save_as_pt</span><span class="p">(</span><span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span><span class="p">,</span> <span class="n">sentences</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;for example providing a small sentence&quot;</span><span class="p">,</span> <span class="s2">&quot;we can add multiple sentences&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
model file is saved to  /Volumes/workplace/upload_content/all-MiniLM-L6-v2.pt
zip file is saved to  /Volumes/workplace/upload_content/all-MiniLM-L6-v2.zip

</pre></div></div>
</div>
</section>
<section id="Step-2:-Upload-the-saved-torchScript-model-in-Opensearch">
<h2>Step 2: Upload the saved torchScript model in Opensearch<a class="headerlink" href="#Step-2:-Upload-the-saved-torchScript-model-in-Opensearch" title="Permalink to this heading">¶</a></h2>
<p>In the last step we saved a sentence transformer model in torchScript format. Now we will upload that model in opensearch cluster. To do that we can take help of <code class="docutils literal notranslate"><span class="pre">upload_model</span></code> method in <code class="docutils literal notranslate"><span class="pre">opensearch-py-ml</span></code> plugin.</p>
<p>To upload model, we need the zip file we just saved in the last step and a model config file. Example of Model config file content can be:</p>
<p>{ “name”: “all-MiniLM-L6-v2”, “version”: “1.0.0”, “description”: “test model”, “model_format”: “TORCH_SCRIPT”, “model_config”: { “model_type”: “bert”, “embedding_dimension”: 384, “framework_type”: “sentence_transformers” } }</p>
<p><code class="docutils literal notranslate"><span class="pre">model_format</span></code> needs to be <code class="docutils literal notranslate"><span class="pre">TORCH_SCRIPT</span></code> so that internal system will look for the corresponding <code class="docutils literal notranslate"><span class="pre">.pt</span></code> file from the zip folder.</p>
<p>Please refer to this doc: <a class="reference external" href="https://github.com/opensearch-project/ml-commons/blob/2.x/docs/model_serving_framework/text_embedding_model_examples.md">https://github.com/opensearch-project/ml-commons/blob/2.x/docs/model_serving_framework/text_embedding_model_examples.md</a></p>
<p>Documentation for the method: <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/reference/api/ml_commons_upload_api.html#opensearch_py_ml.ml_commons.MLCommonClient.upload_model">https://opensearch-project.github.io/opensearch-py-ml/reference/api/ml_commons_upload_api.html#opensearch_py_ml.ml_commons.MLCommonClient.upload_model</a></p>
<p>Related demo notebook about ml-commons plugin integration: <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/examples/demo_ml_commons_integration.html">https://opensearch-project.github.io/opensearch-py-ml/examples/demo_ml_commons_integration.html</a></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><br/><span></span><span class="n">model_config_path</span> <span class="o">=</span> <span class="s1">&#39;/Volumes/workplace/upload_content/model_config_torchscript.json&#39;</span>
<span class="n">ml_client</span><span class="o">.</span><span class="n">upload_model</span><span class="p">(</span> <span class="n">model_path</span><span class="p">,</span> <span class="n">model_config_path</span><span class="p">,</span> <span class="n">isVerbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Total number of chunks 10
Sha1 value of the model file:  0a8eabed8c09b09b588e9d4c3d42e61c90e524d014be5547b73db75e77185576
Model meta data was created successfully. Model Id:  SGHQ9IUBTo3f8n5RC3ZH
uploading chunk 1 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 2 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 3 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 4 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 5 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 6 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 7 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 8 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 9 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 10 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
Model uploaded successfully
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;SGHQ9IUBTo3f8n5RC3ZH&#39;
</pre></div></div>
</div>
</section>
<section id="Step-3:Save-model-in-Onnx-format">
<h2>Step 3:Save model in Onnx format<a class="headerlink" href="#Step-3:Save-model-in-Onnx-format" title="Permalink to this heading">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">Opensearch-py-ml</span></code> plugin provides method <code class="docutils literal notranslate"><span class="pre">save_as_onnx</span></code> which will trace a model in ONNX format and save the model in a zip file in your filesystem.</p>
<p>Detailed documentation: <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.save_as_onnx.html#opensearch_py_ml.ml_models.SentenceTransformerModel.save_as_onnx">https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.save_as_onnx.html#opensearch_py_ml.ml_models.SentenceTransformerModel.save_as_onnx</a></p>
<p>Users need to provide a model id from sentence transformers (an example: <code class="docutils literal notranslate"><span class="pre">sentence-transformers/all-MiniLM-L6-v2</span></code>). <code class="docutils literal notranslate"><span class="pre">save_as_onnx</span></code> will download the model in filesystem and then trace the model.</p>
<p>after tracing the model (a .onnx file will be generated), <code class="docutils literal notranslate"><span class="pre">save_as_onnx</span></code> method zips <code class="docutils literal notranslate"><span class="pre">tokenizers.json</span></code> and torchScript (<code class="docutils literal notranslate"><span class="pre">.onnx</span></code>) file and saves in the file system.</p>
<p>User can upload that model to opensearch to generate embedding.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pre_trained_model</span> <span class="o">=</span> <span class="n">SentenceTransformerModel</span><span class="p">(</span><span class="n">folder_path</span> <span class="o">=</span> <span class="s1">&#39;/Volumes/workplace/upload_content/&#39;</span><span class="p">,</span> <span class="n">overwrite</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">model_path_onnx</span> <span class="o">=</span> <span class="n">pre_trained_model</span><span class="o">.</span><span class="n">save_as_onnx</span><span class="p">(</span><span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;sentence-transformers/all-MiniLM-L6-v2&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
ONNX opset version set to: 16
Loading pipeline (model: sentence-transformers/all-MiniLM-L6-v2, tokenizer: sentence-transformers/all-MiniLM-L6-v2)
Creating folder /Volumes/workplace/upload_content/onnx
Using framework PyTorch: 1.13.1
Found input input_ids with shape: {0: &#39;batch&#39;, 1: &#39;sequence&#39;}
Found input token_type_ids with shape: {0: &#39;batch&#39;, 1: &#39;sequence&#39;}
Found input attention_mask with shape: {0: &#39;batch&#39;, 1: &#39;sequence&#39;}
Found output output_0 with shape: {0: &#39;batch&#39;, 1: &#39;sequence&#39;}
Found output output_1 with shape: {0: &#39;batch&#39;}
Ensuring inputs are in correct order
position_ids is not present in the generated input list.
Generated inputs order: [&#39;input_ids&#39;, &#39;attention_mask&#39;, &#39;token_type_ids&#39;]
zip file is saved to  /Volumes/workplace/upload_content/all-MiniLM-L6-v2.zip

</pre></div></div>
</div>
</section>
<section id="Step-4:-Upload-the-saved-Onnx-model-in-Opensearch">
<h2>Step 4: Upload the saved Onnx model in Opensearch<a class="headerlink" href="#Step-4:-Upload-the-saved-Onnx-model-in-Opensearch" title="Permalink to this heading">¶</a></h2>
<p>In the last step we saved a sentence transformer model in ONNX format. Now we will upload that model in opensearch cluster. To do that we can take help of <code class="docutils literal notranslate"><span class="pre">upload_model</span></code> method in <code class="docutils literal notranslate"><span class="pre">opensearch-py-ml</span></code> plugin.</p>
<p>To upload model, we need the zip file we just saved in the last step and a model config file. Example of Model config file content can be:</p>
<p>{ “name”: “all-MiniLM-L6-v2”, “version”: “1.0.0”, “description”: “test model”, “model_format”: “ONNX”, “model_config”: { “model_type”: “bert”, “embedding_dimension”: 384, “framework_type”: “sentence_transformers”, “pooling_mode”:“mean”, “normalize_result”:“true” } }</p>
<p><code class="docutils literal notranslate"><span class="pre">model_format</span></code> needs to be <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> so that internal system will look for the corresponding <code class="docutils literal notranslate"><span class="pre">.onnx</span></code> file from the zip folder.</p>
<p>Please refer to this doc: <a class="reference external" href="https://github.com/opensearch-project/ml-commons/blob/2.x/docs/model_serving_framework/text_embedding_model_examples.md">https://github.com/opensearch-project/ml-commons/blob/2.x/docs/model_serving_framework/text_embedding_model_examples.md</a></p>
<p>Documentation for the method: <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/reference/api/ml_commons_upload_api.html#opensearch_py_ml.ml_commons.MLCommonClient.upload_model">https://opensearch-project.github.io/opensearch-py-ml/reference/api/ml_commons_upload_api.html#opensearch_py_ml.ml_commons.MLCommonClient.upload_model</a></p>
<p>Related demo notebook about ml-commons plugin integration: <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/examples/demo_ml_commons_integration.html">https://opensearch-project.github.io/opensearch-py-ml/examples/demo_ml_commons_integration.html</a></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><br/><span></span><span class="n">model_config_path</span> <span class="o">=</span> <span class="s1">&#39;/Volumes/workplace/upload_content/model_config.json&#39;</span>
<span class="n">ml_client</span><span class="o">.</span><span class="n">upload_model</span><span class="p">(</span> <span class="n">model_path_onnx</span><span class="p">,</span> <span class="n">model_config_path</span><span class="p">,</span> <span class="n">isVerbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Total number of chunks 10
Sha1 value of the model file:  8faa075a1693f3734f956a8c8e0de755ad4142b59df6d605298879b0dab31308
Model meta data was created successfully. Model Id:  YWHS9IUBTo3f8n5ReXas
uploading chunk 1 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 2 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 3 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 4 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 5 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 6 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 7 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 8 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 9 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 10 of 10
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
Model uploaded successfully
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;YWHS9IUBTo3f8n5ReXas&#39;
</pre></div></div>
</div>
</section>
<section id="Step-5:-Generate-Sentence-Embedding">
<h2>Step 5: Generate Sentence Embedding<a class="headerlink" href="#Step-5:-Generate-Sentence-Embedding" title="Permalink to this heading">¶</a></h2>
<p>Now after loading these models in memory, we can generate embedding for sentences. We can provide a list of sentences to get a list of embedding for the sentences.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Now using this model we can generate sentence embedding.</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">input_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;first sentence&quot;</span><span class="p">,</span> <span class="s2">&quot;second sentence&quot;</span><span class="p">]</span>

<span class="c1"># generated embedding from torchScript</span>

<span class="n">embedding_output_torch</span> <span class="o">=</span> <span class="n">ml_client</span><span class="o">.</span><span class="n">generate_embedding</span><span class="p">(</span><span class="s2">&quot;SGHQ9IUBTo3f8n5RC3ZH&quot;</span><span class="p">,</span> <span class="n">input_sentences</span><span class="p">)</span>

<span class="c1">#just taking embedding for the first sentence</span>
<span class="n">data_torch</span> <span class="o">=</span> <span class="n">embedding_output_torch</span><span class="p">[</span><span class="s2">&quot;inference_results&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;output&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;data&quot;</span><span class="p">]</span>

<span class="c1"># generated embedding from onnx</span>

<span class="n">embedding_output_onnx</span> <span class="o">=</span> <span class="n">ml_client</span><span class="o">.</span><span class="n">generate_embedding</span><span class="p">(</span><span class="s2">&quot;YWHS9IUBTo3f8n5ReXas&quot;</span><span class="p">,</span> <span class="n">input_sentences</span><span class="p">)</span>

<span class="c1">#just taking embedding for the first sentence</span>
<span class="n">data_onnx</span> <span class="o">=</span> <span class="n">embedding_output_onnx</span><span class="p">[</span><span class="s2">&quot;inference_results&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;output&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;data&quot;</span><span class="p">]</span>

<span class="c1">## Now we can check if there&#39;s any significant difference between two outputs</span>

<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">data_torch</span><span class="p">,</span> <span class="n">data_onnx</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-03</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">))</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
None
</pre></div></div>
</div>
</section>
</section>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="demo_ml_commons_integration.html" class="btn btn-neutral float-left" title="Demo Notebook for MLCommons Integration" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2023, Opensearch.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>