<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Demo Notebook to trace Sentence Transformers model &mdash; Opensearch-py-ml 1.0.0 documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/plot_directive.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
        <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Demo Notebook for MLCommons Integration" href="demo_ml_commons_integration.html" />
    <link rel="prev" title="Demo Notebook for Sentence Transformer Model Training, Saving and Uploading to OpenSearch" href="demo_transformer_model_train_save_upload_to_openSearch.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Opensearch-py-ml
          </a>
              <div class="version">
                1.0.0
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../reference/index.html">API Reference</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="index.html#demo-notebooks-for-data-exploration-panda-like-dataframe">Demo notebooks for Data Exploration Panda like DataFrame</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html#demo-notebooks-for-model-training-and-tracing">Demo notebooks for Model Training and Tracing</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="demo_transformer_model_train_save_upload_to_openSearch.html">Demo Notebook for Sentence Transformer Model Training, Saving and Uploading to OpenSearch</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Demo Notebook to trace Sentence Transformers model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#download-notebook">download notebook</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#demo-notebooks-for-ml-commons-plugin-integration">Demo notebooks for ML Commons plugin integration</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Opensearch-py-ml</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Examples</a></li>
      <li class="breadcrumb-item active">Demo Notebook to trace Sentence Transformers model</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/examples/demo_tracing_model_torchscript_onnx.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Demo-Notebook-to-trace-Sentence-Transformers-model">
<h1>Demo Notebook to trace Sentence Transformers model<a class="headerlink" href="#Demo-Notebook-to-trace-Sentence-Transformers-model" title="Permalink to this heading"></a></h1>
<section id="download-notebook">
<h2><a class="reference external" href="https://github.com/opensearch-project/opensearch-py-ml/blob/main/docs/source/examples/demo_tracing_model_torchscript_onnx.ipynb">download notebook</a><a class="headerlink" href="#download-notebook" title="Permalink to this heading"></a></h2>
<p>This notebook provides a walkthrough guidance for users to trace models from Sentence Transformers in torchScript and onnx format. After tracing the model customer can upload the model to opensearch and generate embeddings.</p>
<p>Remember, tracing model in torchScript or Onnx format at just two different options. We don’t need to trace model in both ways. Here in our notebook we just want to show both ways.</p>
<p>Step 0: Import packages and set up client</p>
<p>Step 1: Save model in torchScript format</p>
<p>Step 2: Upload the saved torchScript model in Opensearch</p>
<p>[The following steps are optional, just showing uploading model in both ways and comparing the both embedding output]</p>
<p>Step 3: Save model in Onnx format</p>
<p>Step 4: Upload the saved Onnx model in Opensearch</p>
<p>Step 5: Generate Sentence Embedding with uploaded models</p>
<section id="Step-0:-Import-packages-and-set-up-client">
<h3>Step 0: Import packages and set up client<a class="headerlink" href="#Step-0:-Import-packages-and-set-up-client" title="Permalink to this heading"></a></h3>
<p>Install required packages for opensearch_py_ml.sentence_transformer_model Install <code class="docutils literal notranslate"><span class="pre">opensearchpy</span></code> and <code class="docutils literal notranslate"><span class="pre">opensearch-py-ml</span></code> through pypi</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="ch">#!pip install opensearch-py opensearch-py-ml</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">DeprecationWarning</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">FutureWarning</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;Unverified HTTPS request&quot;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;TracerWarning: torch.tensor&quot;</span><span class="p">)</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">,</span> <span class="n">message</span><span class="o">=</span><span class="s2">&quot;using SSL with verify_certs=False is insecure.&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">opensearch_py_ml</span> <span class="k">as</span> <span class="nn">oml</span>
<span class="kn">from</span> <span class="nn">opensearchpy</span> <span class="kn">import</span> <span class="n">OpenSearch</span>
<span class="kn">from</span> <span class="nn">opensearch_py_ml.ml_models</span> <span class="kn">import</span> <span class="n">SentenceTransformerModel</span>
<span class="c1"># import mlcommon to later upload the model to OpenSearch Cluster</span>
<span class="kn">from</span> <span class="nn">opensearch_py_ml.ml_commons</span> <span class="kn">import</span> <span class="n">MLCommonClient</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">CLUSTER_URL</span> <span class="o">=</span> <span class="s1">&#39;https://localhost:9200&#39;</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_os_client</span><span class="p">(</span><span class="n">cluster_url</span> <span class="o">=</span> <span class="n">CLUSTER_URL</span><span class="p">,</span>
                  <span class="n">username</span><span class="o">=</span><span class="s1">&#39;admin&#39;</span><span class="p">,</span>
                  <span class="n">password</span><span class="o">=</span><span class="s1">&#39;admin&#39;</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Get OpenSearch client</span>
<span class="sd">    :param cluster_url: cluster URL like https://ml-te-netwo-1s12ba42br23v-ff1736fa7db98ff2.elb.us-west-2.amazonaws.com:443</span>
<span class="sd">    :return: OpenSearch client</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="n">client</span> <span class="o">=</span> <span class="n">OpenSearch</span><span class="p">(</span>
        <span class="n">hosts</span><span class="o">=</span><span class="p">[</span><span class="n">cluster_url</span><span class="p">],</span>
        <span class="n">http_auth</span><span class="o">=</span><span class="p">(</span><span class="n">username</span><span class="p">,</span> <span class="n">password</span><span class="p">),</span>
        <span class="n">verify_certs</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">client</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">client</span> <span class="o">=</span> <span class="n">get_os_client</span><span class="p">()</span>

<span class="c1">#connect to ml_common client with OpenSearch client</span>
<span class="kn">import</span> <span class="nn">opensearch_py_ml</span> <span class="k">as</span> <span class="nn">oml</span>
<span class="kn">from</span> <span class="nn">opensearch_py_ml.ml_commons</span> <span class="kn">import</span> <span class="n">MLCommonClient</span>
<span class="n">ml_client</span> <span class="o">=</span> <span class="n">MLCommonClient</span><span class="p">(</span><span class="n">client</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Step-1:-Save-model-in-torchScript-format">
<h3>Step 1: Save model in torchScript format<a class="headerlink" href="#Step-1:-Save-model-in-torchScript-format" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Opensearch-py-ml</span></code> plugin provides method <code class="docutils literal notranslate"><span class="pre">save_as_pt</span></code> which will trace a model in torchScript format and save the model in a zip file in your filesystem.</p>
<p>Detailed documentation: <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.save_as_pt.html#opensearch_py_ml.ml_models.SentenceTransformerModel.save_as_pt">https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.save_as_pt.html#opensearch_py_ml.ml_models.SentenceTransformerModel.save_as_pt</a></p>
<p>Users need to provide a model id from sentence transformers (an example: <code class="docutils literal notranslate"><span class="pre">sentence-transformers/msmarco-distilbert-base-tas-b</span></code>). This model id is a huggingface model id. Exaample: <a class="reference external" href="https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b">https://huggingface.co/sentence-transformers/msmarco-distilbert-base-tas-b</a></p>
<p><code class="docutils literal notranslate"><span class="pre">save_as_pt</span></code> will download the model in filesystem and then trace the model with the given input strings.</p>
<p>To get more direction about dummy input string please check this url: <a class="reference external" href="https://huggingface.co/docs/transformers/torchscript#dummy-inputs-and-standard-lengths">https://huggingface.co/docs/transformers/torchscript#dummy-inputs-and-standard-lengths</a></p>
<p>after tracing the model (a .pt file will be generated), <code class="docutils literal notranslate"><span class="pre">save_as_pt</span></code> method zips <code class="docutils literal notranslate"><span class="pre">tokenizers.json</span></code> and torchScript (<code class="docutils literal notranslate"><span class="pre">.pt</span></code>) file and saves in the file system.</p>
<p>User can upload that model to opensearch to generate embedding.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pre_trained_model</span> <span class="o">=</span> <span class="n">SentenceTransformerModel</span><span class="p">(</span><span class="n">folder_path</span> <span class="o">=</span> <span class="s1">&#39;/Volumes/workplace/upload_content/&#39;</span><span class="p">,</span> <span class="n">overwrite</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="n">pre_trained_model</span><span class="o">.</span><span class="n">save_as_pt</span><span class="p">(</span><span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;sentence-transformers/msmarco-distilbert-base-tas-b&quot;</span><span class="p">,</span> <span class="n">sentences</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;for example providing a small sentence&quot;</span><span class="p">,</span> <span class="s2">&quot;we can add multiple sentences&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
model file is saved to  /Volumes/workplace/upload_content/msmarco-distilbert-base-tas-b.pt
zip file is saved to  /Volumes/workplace/upload_content/msmarco-distilbert-base-tas-b.zip

</pre></div></div>
</div>
</section>
<section id="Step-2:-Upload-the-saved-torchScript-model-in-Opensearch">
<h3>Step 2: Upload the saved torchScript model in Opensearch<a class="headerlink" href="#Step-2:-Upload-the-saved-torchScript-model-in-Opensearch" title="Permalink to this heading"></a></h3>
<p>In the last step we saved a sentence transformer model in torchScript format. Now we will upload that model in opensearch cluster. To do that we can take help of <code class="docutils literal notranslate"><span class="pre">upload_model</span></code> method in <code class="docutils literal notranslate"><span class="pre">opensearch-py-ml</span></code> plugin.</p>
<p>To upload model, we need the zip file we just saved in the last step and a model config file. Example of Model config file content can be:</p>
<p>{ “name”: “sentence-transformers/msmarco-distilbert-base-tas-b”, “version”: “1.0.0”, “description”: “This is a port of the DistilBert TAS-B Model to sentence-transformers model: It maps sentences &amp; paragraphs to a 768 dimensional dense vector space and is optimized for the task of semantic search.”, “model_format”: “TORCH_SCRIPT”, “model_config”: { “model_type”: “distilbert”, “embedding_dimension”: 768, “framework_type”: “sentence_transformers” } }</p>
<p><code class="docutils literal notranslate"><span class="pre">model_format</span></code> needs to be <code class="docutils literal notranslate"><span class="pre">TORCH_SCRIPT</span></code> so that internal system will look for the corresponding <code class="docutils literal notranslate"><span class="pre">.pt</span></code> file from the zip folder.</p>
<p>Please refer to this doc: <a class="reference external" href="https://github.com/opensearch-project/ml-commons/blob/2.x/docs/model_serving_framework/text_embedding_model_examples.md">https://github.com/opensearch-project/ml-commons/blob/2.x/docs/model_serving_framework/text_embedding_model_examples.md</a></p>
<p>Documentation for the method: <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/reference/api/ml_commons_upload_api.html#opensearch_py_ml.ml_commons.MLCommonClient.upload_model">https://opensearch-project.github.io/opensearch-py-ml/reference/api/ml_commons_upload_api.html#opensearch_py_ml.ml_commons.MLCommonClient.upload_model</a></p>
<p>Related demo notebook about ml-commons plugin integration: <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/examples/demo_ml_commons_integration.html">https://opensearch-project.github.io/opensearch-py-ml/examples/demo_ml_commons_integration.html</a></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><br/><span></span><span class="n">model_config_path_torch</span> <span class="o">=</span> <span class="s1">&#39;/Volumes/workplace/upload_content/model_config_torchscript.json&#39;</span>
<span class="n">ml_client</span><span class="o">.</span><span class="n">upload_model</span><span class="p">(</span> <span class="n">model_path</span><span class="p">,</span> <span class="n">model_config_path_torch</span><span class="p">,</span> <span class="n">isVerbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Total number of chunks 27
Sha1 value of the model file:  9614778c36d4c64c5dd77cdedde86e219086dbecb5ace7877f697b1af28d642a
Model meta data was created successfully. Model Id:  X-LeKIYB7pteJtrc9XHu
uploading chunk 1 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 2 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 3 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 4 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 5 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 6 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 7 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 8 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 9 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 10 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 11 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 12 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 13 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 14 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 15 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 16 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 17 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 18 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 19 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 20 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 21 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 22 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 23 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 24 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 25 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 26 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 27 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
Model uploaded successfully
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;X-LeKIYB7pteJtrc9XHu&#39;
</pre></div></div>
</div>
</section>
<section id="Step-3:Save-model-in-Onnx-format">
<h3>Step 3:Save model in Onnx format<a class="headerlink" href="#Step-3:Save-model-in-Onnx-format" title="Permalink to this heading"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Opensearch-py-ml</span></code> plugin provides method <code class="docutils literal notranslate"><span class="pre">save_as_onnx</span></code> which will trace a model in ONNX format and save the model in a zip file in your filesystem.</p>
<p>Detailed documentation: <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.save_as_onnx.html#opensearch_py_ml.ml_models.SentenceTransformerModel.save_as_onnx">https://opensearch-project.github.io/opensearch-py-ml/reference/api/sentence_transformer.save_as_onnx.html#opensearch_py_ml.ml_models.SentenceTransformerModel.save_as_onnx</a></p>
<p>Users need to provide a model id from sentence transformers (an example: <code class="docutils literal notranslate"><span class="pre">sentence-transformers/msmarco-distilbert-base-tas-b</span></code>). <code class="docutils literal notranslate"><span class="pre">save_as_onnx</span></code> will download the model in filesystem and then trace the model.</p>
<p>after tracing the model (a .onnx file will be generated), <code class="docutils literal notranslate"><span class="pre">save_as_onnx</span></code> method zips <code class="docutils literal notranslate"><span class="pre">tokenizers.json</span></code> and torchScript (<code class="docutils literal notranslate"><span class="pre">.onnx</span></code>) file and saves in the file system.</p>
<p>User can upload that model to opensearch to generate embedding.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pre_trained_model</span> <span class="o">=</span> <span class="n">SentenceTransformerModel</span><span class="p">(</span><span class="n">folder_path</span> <span class="o">=</span> <span class="s1">&#39;/Volumes/workplace/upload_content/&#39;</span><span class="p">,</span> <span class="n">overwrite</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">model_path_onnx</span> <span class="o">=</span> <span class="n">pre_trained_model</span><span class="o">.</span><span class="n">save_as_onnx</span><span class="p">(</span><span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;sentence-transformers/msmarco-distilbert-base-tas-b&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
ONNX opset version set to: 15
Loading pipeline (model: sentence-transformers/msmarco-distilbert-base-tas-b, tokenizer: sentence-transformers/msmarco-distilbert-base-tas-b)
Creating folder /Volumes/workplace/upload_content/onnx
Using framework PyTorch: 1.13.1
Found input input_ids with shape: {0: &#39;batch&#39;, 1: &#39;sequence&#39;}
Found input attention_mask with shape: {0: &#39;batch&#39;, 1: &#39;sequence&#39;}
Found output output_0 with shape: {0: &#39;batch&#39;, 1: &#39;sequence&#39;}
Ensuring inputs are in correct order
head_mask is not present in the generated input list.
Generated inputs order: [&#39;input_ids&#39;, &#39;attention_mask&#39;]
zip file is saved to  /Volumes/workplace/upload_content/msmarco-distilbert-base-tas-b.zip

</pre></div></div>
</div>
</section>
<section id="Step-4:-Upload-the-saved-Onnx-model-in-Opensearch">
<h3>Step 4: Upload the saved Onnx model in Opensearch<a class="headerlink" href="#Step-4:-Upload-the-saved-Onnx-model-in-Opensearch" title="Permalink to this heading"></a></h3>
<p>In the last step we saved a sentence transformer model in ONNX format. Now we will upload that model in opensearch cluster. To do that we can take help of <code class="docutils literal notranslate"><span class="pre">upload_model</span></code> method in <code class="docutils literal notranslate"><span class="pre">opensearch-py-ml</span></code> plugin.</p>
<p>To upload model, we need the zip file we just saved in the last step and a model config file. Example of Model config file content can be:</p>
<p>{ “name”: “sentence-transformers/msmarco-distilbert-base-tas-b”, “version”: “1.0.0”, “description”: “This is a port of the DistilBert TAS-B Model to sentence-transformers model: It maps sentences &amp; paragraphs to a 768 dimensional dense vector space and is optimized for the task of semantic search.”, “model_format”: “ONNX”, “model_config”: { “model_type”: “distilbert”, “embedding_dimension”: 768, “framework_type”: “sentence_transformers”, “pooling_mode”:“cls”, “normalize_result”:“false” } }</p>
<p><code class="docutils literal notranslate"><span class="pre">model_format</span></code> needs to be <code class="docutils literal notranslate"><span class="pre">ONNX</span></code> so that internal system will look for the corresponding <code class="docutils literal notranslate"><span class="pre">.onnx</span></code> file from the zip folder.</p>
<p>Please refer to this doc: <a class="reference external" href="https://github.com/opensearch-project/ml-commons/blob/2.x/docs/model_serving_framework/text_embedding_model_examples.md">https://github.com/opensearch-project/ml-commons/blob/2.x/docs/model_serving_framework/text_embedding_model_examples.md</a></p>
<p>Documentation for the method: <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/reference/api/ml_commons_upload_api.html#opensearch_py_ml.ml_commons.MLCommonClient.upload_model">https://opensearch-project.github.io/opensearch-py-ml/reference/api/ml_commons_upload_api.html#opensearch_py_ml.ml_commons.MLCommonClient.upload_model</a></p>
<p>Related demo notebook about ml-commons plugin integration: <a class="reference external" href="https://opensearch-project.github.io/opensearch-py-ml/examples/demo_ml_commons_integration.html">https://opensearch-project.github.io/opensearch-py-ml/examples/demo_ml_commons_integration.html</a></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><br/><span></span><span class="n">model_config_path_onnx</span> <span class="o">=</span> <span class="s1">&#39;/Volumes/workplace/upload_content/model_config.json&#39;</span>
<span class="n">ml_client</span><span class="o">.</span><span class="n">upload_model</span><span class="p">(</span> <span class="n">model_path_onnx</span><span class="p">,</span> <span class="n">model_config_path_onnx</span><span class="p">,</span> <span class="n">isVerbose</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Total number of chunks 27
Sha1 value of the model file:  f0935b34243937fc129165fc189bb29b226f5dac9cfde3cb1150eae35c4622e2
Model meta data was created successfully. Model Id:  YeLhKIYB7pteJtrcHXFV
uploading chunk 1 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 2 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 3 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 4 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 5 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 6 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 7 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 8 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 9 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 10 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 11 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 12 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 13 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 14 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 15 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 16 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 17 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 18 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 19 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 20 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 21 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 22 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 23 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 24 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 25 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 26 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
uploading chunk 27 of 27
Model id: {&#39;status&#39;: &#39;Uploaded&#39;}
Model uploaded successfully
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;YeLhKIYB7pteJtrcHXFV&#39;
</pre></div></div>
</div>
</section>
<section id="Step-5:-Generate-Sentence-Embedding">
<h3>Step 5: Generate Sentence Embedding<a class="headerlink" href="#Step-5:-Generate-Sentence-Embedding" title="Permalink to this heading"></a></h3>
<p>Now after loading these models in memory, we can generate embedding for sentences. We can provide a list of sentences to get a list of embedding for the sentences.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Now using this model we can generate sentence embedding.</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">input_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;first sentence&quot;</span><span class="p">,</span> <span class="s2">&quot;second sentence&quot;</span><span class="p">]</span>

<span class="c1"># generated embedding from torchScript</span>

<span class="n">embedding_output_torch</span> <span class="o">=</span> <span class="n">ml_client</span><span class="o">.</span><span class="n">generate_embedding</span><span class="p">(</span><span class="s2">&quot;X-LeKIYB7pteJtrc9XHu&quot;</span><span class="p">,</span> <span class="n">input_sentences</span><span class="p">)</span>

<span class="c1">#just taking embedding for the first sentence</span>
<span class="n">data_torch</span> <span class="o">=</span> <span class="n">embedding_output_torch</span><span class="p">[</span><span class="s2">&quot;inference_results&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;output&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;data&quot;</span><span class="p">]</span>

<span class="c1"># generated embedding from onnx</span>

<span class="n">embedding_output_onnx</span> <span class="o">=</span> <span class="n">ml_client</span><span class="o">.</span><span class="n">generate_embedding</span><span class="p">(</span><span class="s2">&quot;YeLhKIYB7pteJtrcHXFV&quot;</span><span class="p">,</span> <span class="n">input_sentences</span><span class="p">)</span>

<span class="c1">#just taking embedding for the first sentence</span>
<span class="n">data_onnx</span> <span class="o">=</span> <span class="n">embedding_output_onnx</span><span class="p">[</span><span class="s2">&quot;inference_results&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;output&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;data&quot;</span><span class="p">]</span>

<span class="c1">## Now we can check if there&#39;s any significant difference between two outputs</span>

<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">data_torch</span><span class="p">,</span> <span class="n">data_onnx</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-03</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
None
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="demo_transformer_model_train_save_upload_to_openSearch.html" class="btn btn-neutral float-left" title="Demo Notebook for Sentence Transformer Model Training, Saving and Uploading to OpenSearch" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="demo_ml_commons_integration.html" class="btn btn-neutral float-right" title="Demo Notebook for MLCommons Integration" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, Opensearch.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>